{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_YyhKn_iUGR"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "91SNqMuGiduB"
   },
   "outputs": [],
   "source": [
    "trainset = h5py.File(\"../data/train.h5\", \"r\")\n",
    "validateset = h5py.File(\"../data/val.h5\", \"r\")\n",
    "testset = h5py.File(\"../data/test.h5\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0cBTSLt8YyVk",
    "outputId": "81541b59-dd7e-4430-e23f-25a1218fa423"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agbd:\t(25036,)\n",
      "cloud:\t(25036, 15, 15, 1)\n",
      "images:\t(25036, 15, 15, 12)\n",
      "lat:\t(25036, 15, 15, 1)\n",
      "lon:\t(25036, 15, 15, 1)\n",
      "scl:\t(25036, 15, 15, 1)\n"
     ]
    }
   ],
   "source": [
    "for feature in trainset:\n",
    "    print(f\"{feature}:\\t{trainset[feature][:].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsFR16OG968x"
   },
   "source": [
    "## Image Regression - Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qZKFc7nzbzBI",
    "outputId": "7bb7220d-1091-4a1a-ff05-e3a9882a7651"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6350e4dfd0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCt7wrp6qIvB"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_patches, n_features, n_heads, dtype=torch.float64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm((n_patches, n_features), elementwise_affine=False, dtype=dtype)\n",
    "        self.msa = nn.MultiheadAttention(n_features, n_heads, batch_first=True, dtype=dtype)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_features, n_features, dtype=dtype),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(n_features, n_features, dtype=dtype),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.layer_norm(input)\n",
    "        msa_output  = self.msa(x, x, x)[0]\n",
    "        mlp_input   = msa_output + input\n",
    "        mlp_output  = self.mlp(self.layer_norm(mlp_input))\n",
    "        output      = mlp_output + mlp_input\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkeaTiQic98y"
   },
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, n_channels, n_enc_blocks=1, dtype=torch.float64):\n",
    "        super().__init__()\n",
    "\n",
    "        n_patches = (image_size // patch_size)**2\n",
    "        n_features = patch_size**2 * n_channels\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.learnable_emb = nn.Parameter(torch.zeros(1, n_features))\n",
    "        self.pos = nn.Parameter(self.pos_emb(n_patches+1, n_features))\n",
    " \n",
    "        self.encoder = nn.ModuleList([\n",
    "            Encoder(n_patches+1, n_features, patch_size, dtype)\n",
    "            for _ in range(n_enc_blocks)\n",
    "        ])\n",
    "\n",
    "        self.linear = nn.Linear(n_features, 1, dtype=dtype)\n",
    "\n",
    "        self.MSE = nn.MSELoss(reduction='none')\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = torch.from_numpy(input / 10000)\n",
    "\n",
    "        patches = self.patched(input, self.patch_size)\n",
    "\n",
    "        patches = torch.cat([self.learnable_emb.unsqueeze(0).repeat(len(patches),1,1), patches], 1)\n",
    "        enc_input = patches + self.pos\n",
    "\n",
    "        for enc_block in self.encoder:\n",
    "            enc_input = enc_block(enc_input)\n",
    "\n",
    "        linear_input = enc_input[:,0]\n",
    "        output = self.linear(linear_input)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def fit(self, inputs, targets, epochs, batch_size, lr=1e-2):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        n_samples = len(inputs)\n",
    "        inputs  = [ inputs[i:i+batch_size] for i in range(0,len(inputs) ,batch_size)]\n",
    "        targets = [targets[i:i+batch_size] for i in range(0,len(targets),batch_size)]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "\n",
    "            for input,target in zip(inputs,targets):\n",
    "                optimizer.zero_grad()\n",
    "                loss = self.MSE(self(input), target)\n",
    "                # loss = (self(input) - target).square()\n",
    "                total_loss += loss.sum().item()\n",
    "\n",
    "                loss.backward(torch.ones(batch_size).unsqueeze(1))\n",
    "                optimizer.step()\n",
    "\n",
    "            total_loss /= n_samples\n",
    "\n",
    "            if (total_loss < 100):\n",
    "                print(\"---------------------\")\n",
    "                print(\"Num of epoches:\", epoch)\n",
    "                print(\"\\tMSE  = \", total_loss)\n",
    "                print(\"\\tRMSE = \", total_loss**(1/2))\n",
    "                print(\"---------------------\")\n",
    "                break\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}: \\tMSE  = {total_loss:.4f} \\tRSME = {total_loss**(1/2):.4f}\")\n",
    "\n",
    "\n",
    "    def patched(self, images, patch_size):\n",
    "        # Input:  (n_samples, h_image, w_image, n_channels), (h_patch, w_patch)\n",
    "        # Output: (n_samples, n_patches, n_channels*h_patch*w_patch)\n",
    "\n",
    "        (n_samples, h_image, _, n_channels) = images.shape\n",
    "        h_patch = w_patch = patch_size\n",
    "\n",
    "        flattened_patches = (\n",
    "            images.reshape(n_samples, h_image // h_patch, h_patch, -1, w_patch, n_channels) # Create patch segments\n",
    "            .transpose(2, 3)                                                     # Join the segments, forming patch\n",
    "            .reshape(n_samples, -1, n_channels*h_patch*w_patch)    # Remove excess dimensions and flatten the patch\n",
    "        )\n",
    "        return flattened_patches\n",
    "\n",
    "\n",
    "    def pos_emb(self, n_patches, n_features):\n",
    "        pe = torch.zeros((n_patches,n_features))\n",
    "\n",
    "        i = d_patch = n_features\n",
    "        pos = n_patches\n",
    "\n",
    "        for pos in range(n_patches):\n",
    "            for i in range(0,n_features,2):\n",
    "                pe[pos,i] = np.sin(pos / 10000**(i/d_patch))\n",
    "            for i in range(1,n_features,2):\n",
    "                pe[pos,i] = np.cos(pos / 10000**(i/d_patch))\n",
    "\n",
    "        return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qX5SC4lDpT9J"
   },
   "outputs": [],
   "source": [
    "train_images = trainset['images'][:,:,:,[0,1,7,10]]\n",
    "# Vegetation Index\n",
    "train_images[:,:,:,0] = (trainset['images'][:,:,:,8] - trainset['images'][:,:,:,4]) /\\\n",
    "                        (trainset['images'][:,:,:,8] + trainset['images'][:,:,:,4] + 1e-08)\n",
    "\n",
    "train_agbd = torch.tensor(trainset['agbd'][:], dtype=torch.float64).unsqueeze(1)\n",
    "\n",
    "model = ViT(15, 5, 4, n_enc_blocks=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kg72Xo7ufcym"
   },
   "outputs": [],
   "source": [
    "model.fit(train_images, train_agbd, epochs=30, batch_size=len(train_images), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RSM3yUW-Lykk",
    "outputId": "5b1b9318-9924-4408-f1ea-8f04ed7c3b03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: Loss = tensor([72.7254], dtype=torch.float64, grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.train(train_images, trainset['agbd'][:], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3AjQ_cgl6iG"
   },
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v4Ma42nmqp1h",
    "outputId": "384f513c-4838-4d5a-c859-efd46d26c396"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 43.5597 sec ----\n",
      "---- 0.0017 sec ----\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "for i in range(len(train_images)):\n",
    "    start = time()\n",
    "    model(train_images[i])\n",
    "    end = time()\n",
    "    times.append(end-start)\n",
    "\n",
    "mean_time = sum(times) / len(train_images)\n",
    "print(f\"---- {sum(times):.4f} sec ----\")\n",
    "print(f\"---- {mean_time:.4f} sec ----\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "GCXo54Zv5lH4",
    "WTsrodWN_lDF"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
