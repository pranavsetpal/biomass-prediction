{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_YyhKn_iUGR"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "91SNqMuGiduB"
   },
   "outputs": [],
   "source": [
    "trainset = h5py.File(\"../data/train.h5\", \"r\")\n",
    "validateset = h5py.File(\"../data/val.h5\", \"r\")\n",
    "testset = h5py.File(\"../data/test.h5\", \"r\")\n",
    "\n",
    "train_images = trainset[\"images\"][:] / 10000\n",
    "validate_images = validateset[\"images\"][:] / 10000\n",
    "test_images = testset[\"images\"][:] / 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0cBTSLt8YyVk",
    "outputId": "81541b59-dd7e-4430-e23f-25a1218fa423"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agbd:\t(25036,)\n",
      "cloud:\t(25036, 15, 15, 1)\n",
      "images:\t(25036, 15, 15, 12)\n",
      "lat:\t(25036, 15, 15, 1)\n",
      "lon:\t(25036, 15, 15, 1)\n",
      "scl:\t(25036, 15, 15, 1)\n"
     ]
    }
   ],
   "source": [
    "for feature in trainset:\n",
    "    print(f\"{feature}:\\t{trainset[feature][:].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsFR16OG968x"
   },
   "source": [
    "## Image Regression - Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qZKFc7nzbzBI",
    "outputId": "7bb7220d-1091-4a1a-ff05-e3a9882a7651"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6350e4dfd0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8PFH1bhJGJAl"
   },
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, input):\n",
    "        super().__init__()\n",
    "\n",
    "        n_images,_,_, n_channels = input.shape\n",
    "        images = torch.from_numpy(input / 10000)\n",
    "\n",
    "        images = self.patched(images, (5,5))\n",
    "        patch_len = images.shape[-1]\n",
    "\n",
    "        images += self.pos_emb(images[0].shape)\n",
    "        learnable_emb = nn.Parameter(torch.zeros(n_images, 1, patch_len))\n",
    "        self.input = torch.cat([learnable_emb, images], 1)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(self.input.shape, dtype=self.input.dtype)\n",
    "        self.msa = nn.MultiheadAttention(patch_len, n_channels, dtype=self.input.dtype)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input.shape[-1], input.shape[-1]),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(input.shape[-1], input.shape[-1]),\n",
    "            nn.Dropout()\n",
    "        )\n",
    " \n",
    "\n",
    "    def forward(self, msa_input):\n",
    "        # Transfomrer Encoder\n",
    "        for _ in range(10):\n",
    "            x = self.layer_norm(msa_input)\n",
    "            msa_output  = self.msa(x, x, x)\n",
    "            mlp_input   = msa_output + msa_input\n",
    "            mlp_output  = self.mlp(self.layer_norm(mlp_input))\n",
    "            msa_input   = mlp_output + mlp_input\n",
    "\n",
    "        return msa_input[0]\n",
    "\n",
    "\n",
    "    def patched(self, images, patch_shape):\n",
    "        # Input:  (n_images, h_image, w_image, n_channels), (h_patch, w_patch)\n",
    "        # Output: (n_patches, n_channels*h_patch*w_patch)\n",
    "\n",
    "        (n_images, h_image, _, n_channels) = images.shape\n",
    "        (h_patch, w_patch)                 = patch_shape\n",
    "\n",
    "        flattened_patches = (\n",
    "            images.reshape(n_images, h_image // h_patch, h_patch, -1, w_patch, n_channels) # Create patch segments\n",
    "            .permute(0, 1, 3, 5, 2, 4)                                          # Join the segments, forming patch\n",
    "            .reshape(n_images, -1, n_channels*h_patch*w_patch)    # Remove excess dimensions and flatten the patch\n",
    "        )\n",
    "        return flattened_patches\n",
    "    \n",
    "        \n",
    "    def pos_emb(self, patch_shape):\n",
    "        pe = torch.zeros(patch_shape)\n",
    "\n",
    "        d_patch = patch_shape[1]\n",
    "        (pos, i) = patch_shape\n",
    "\n",
    "        for pos in range(pos):\n",
    "            for i in range(0,i,2):\n",
    "                pe[pos,i] = np.sin(pos / 10000**(i/d_patch))\n",
    "            for i in range(1,i,2):\n",
    "                pe[pos,i] = np.cos(pos / 10000**(i/d_patch))\n",
    "\n",
    "        return pe"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "GCXo54Zv5lH4",
    "WTsrodWN_lDF"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
